[
["index.html", "A brief introduction to econometrics in Stan About", " A brief introduction to econometrics in Stan James Savage 2017-06-19 About These notes are for a half-day short course in econometrics using Stan. The main reason to learn Stan is to fit models that are difficult to fit using other software. Such models might include models with high-dimensional random effects (about which we want to draw inference), models with complex or multi-stage likelihoods, or models with latent data structures. A second reason to learn Stan is that you want to conduct Bayesian analysis on workhorse models; perhaps you have good prior information, or are attracted to the possibility of making probabilistic statements about predictions and parameter estimates. While this second reason is worthwhile, it is not the aim of this course. This course introduces a few workhorse models in order to give you the skills to build richer models that extract the most information from your data. There are three sessions: An introduction to Bayesian reasoning, MCMC/HMC, and Stan. An introduction to Modern Statistical Workflow, using a time-series model as the example. Applying the workflow to a more complex model—in this case, aggregate random coefficients logit. These notes have a few idiosyncracies: Tricks and shortcuts will look like this The code examples live in the models/ folder of the book’s repository, (https://github.com/khakieconomics/shortcourse/models). We use two computing languages in these notes. The first is Stan, a powerful modeling language that allows us to express and estimate probabilistic models with continuous parameter spaces. Stan programs are prefaced with their location in the models/ folder, like so: // models/model_1.stan // ... model code here We also use the R language, for data preparation, calling Stan models, and visualising model results. R programs live in the scripts/ folder; they typically read data from the data/ folder, and liberally use magrittr syntax with dplyr. If this syntax is unfamiliar to you, it is worth taking a look at the excellent vignette to the dplyr package. Like the Stan models, all R code in the book is prefaced with its location in the book’s directory. # scripts/intro.R # ... data work here It is not necessary to be an R aficionado to make the most of these notes. Stan programs can be called from within Stata, Matlab, Mathematica, Julia and Python. If you are more comfortable using those languages than R for data preparation work, then you should be able to implement all the models in this book using those interfaces. Further documentation on calling Stan from other environments is available at http://mc-stan.org/interfaces/. While Stan can be called quite easily from these other programming environments, the R implementation is more fully-fleshed—especially for model checking and post-processing. For this reason we use the R implementation of Stan, rstan in this book. "],
["intro.html", "Session 1 An introduction to Stan 1.1 Why might you want to start learning Bayesian methods? 1.2 Models and inference 1.3 Why use Stan? 1.4 Background: Bayes rule, likelihood and priors 1.5 HMC and Betancourt 1.6 A tour of a Stan program", " Session 1 An introduction to Stan 1.1 Why might you want to start learning Bayesian methods? Learning Bayesian modeling does require a time investment. If you build and estimate statistical models for a living, it is probably an investment worth making, but we should be very explicit about the benefits (and costs) up-front. The benefits are many. Using Bayesian methods, we can take advantage of information that does not necessarily exist in our data (or model structure) in estimating our model. We can combine sources of information in a simple, coherent fashion. Uncertainty in our predictions automatically incorporates uncertainty in parameter estimates. We can define models that are arbitrarily rich—potentially with more parameters than we have data-points— and still expect coherent parameter estimates. We don’t use tests; instead we just check the implied probabilities of outcomes of interest in our estimated model, whose parameters we can give probabilistic interpretations. And perhaps most importantly, using Bayesian methods forces us to understand our models far more deeply than with canned routines. These benefits don’t come free. The approach we advocate in this book—estimating models using full Markov Chain Monte Carlo—can appear slow. Learning Bayesian techniques, and a new programming language, is costly. And some fields have strong frequentist cultures, making communication of Bayesian results an important part of your work. We feel these costs are small relative to the potential gains. Let’s illustrate these benefits with examples. These examples are from real-world applied work by ourselves and our colleagues; hopefully you will see analogies with your own work. Benefit 1: Incorporating knowledge from outside the data When we run a field experiment, we typically want to evaluate the impact of some experimental treatment on an outcome. This impact is known as a treatment effect. Experiments can be costly do perform, limiting the number of observations that we can collect. Consequently, in these small-data studies it is common to have very imprecise estimates of the treatment effect. Bayesian analysis of the experiment can help when there is more information available about the the treatment effect than exists the observations from our experiment, as would be the case of there having been previous studies of the same treatment effect. These previous studies’ results can be incorporated into our study using what is known as a hierarchical prior, resulting in more precise estimates of the treatment effect. For example, imagine you are an education researcher evaluating the impact of a trendy new educational teaching method on test scores. You run a randomized experiment on seventy students at one school and learn that the intervention improved test scores by 34 points on a scale from to 800. Because of the fairly small sample size, this estimate has a standard error of 23.5, and is not “statistically significant” with a p-value of 0.15 (greater than the arbitrary 0.05 threshold used for declaring statistical significance in many fields). This is consistent with a 95% confidence interval of the estimate of (-12.8 to 80.9). If you were to roll out the intervention on a large cohort of students, what would you expect the treatment effect to be? 0—our estimate is not statistically significant? 34? Some other number? Now suppose that because this educational intervention is trendy, it is being experimented on by other researchers. You find that these researchers have achieved the following estimates of the treatment effect from the same treatment (this is Rubin’s famous 8 Schools data (Rubin 1981)): School Treatment effect Standard error A 28.39 14.9 B 7.94 10.2 C -2.75 16.3 D 6.82 11.0 E -0.64 9.4 F 0.63 11.4 G 18.01 10.4 H 12.16 17.6 After seeing these other study results of the same intervention, how might your expectations of a roll-out of the intervention change? It turns out that we can use these study results in re-analyzing our own data in order to get a more precise estimate of the treatment effect in our own study. Doing so reduces the 95% “credibility interval”. Benefit 2: Combining sources of information A similar type of analysis to the is to use Bayesian methods to combine sources of information, as in a meta-analysis or political poll aggregation. The aim of this type of research is to estimate a statistic—for instance, the distribution of political preferences, or a treatment effect—you would expect to find in as-yet untested populations using previous studies as the data source, not the underlying data. The central notion is that these previous studies are noisy estimates of some underlying statistic that applies to the whole population. They are noisy partly because of sampling variability, but also because the studies might differ systematically from one another (political pollsters using different questions, for example). An important assumption is that together, these studies are not systematically biased once we account for observable differences between them. A recent example of this type of analysis is in (Meager 2016), who uses hierarchical Bayesian modeling to obtain estimates of the generalizable treatment effects (and quantile effects) of microcredit expansions on various household measures of financial success. The study makes several contributions, but two highlight the power of a (hierarchical) Bayesian approach. The first is that a biproduct of the Bayesian aggregation procedure is an estimate of the generalizability of a given previous study. That is, the procedure tells us how much we can expect to learn about the impact of a yet-untried microcredit expansion from a given experiment. The second is that using a hierarchical Bayesian aggregation procedure gives us new estimates for the treatment effects in the previous studies. Remember: the estimates from those previous studies are noisy. The technique reduces the noise in these estimates by “borrowing power” from other studies. In the report, several of the “statistically significant” findings in the previous studies lose their “significance” once we adjust them for the fact that similar studies find much smaller (or zero) effects. In a world in which costly policy decisions might be influenced by false discoveries, this feature is appealing. Benefit 3: Dealing with uncertainty consistently in model predictions We often use models to generate predictions or forecasts. There are several types of uncertainty that we ought to be concerned with. The first is sampling variability: even if we have the “perfect model” (we don’t believe such a thing exists) there will remain variation in what we are predicting, either because of pure randomness or measurement error. If we were to use the model for predictions, we should expect the model to be right on average, but not right in every instance. The second source of uncertainty is uncertainty in the unknown parameters in our model. A model itself typically combines “knowns” and unknown variables, and the goal of estimating a model is to draw inference about the value of the unknowns. So long as we only have a limited number of observations, we will be unsure of the precise values of the unknowns; this contributes to uncertainty in our predictions. The third source of uncertainty is uncertainty about whether the model is the right model for the job— is it correctly specified, and are we modeling a stationary (or non-changing) set of relationships? If the model is improperly specified or if the fundamental relationships of the system are changing, our model will not perform well. Importantly, this type of uncertainty is not represented by predictive intervals—it is therefore prudent to treat predictive intervals as the minimum amount of uncertainty that we should have over the outcome. By using Bayesian methods, we automatically deal with the first and second sources of uncertainty, without resorting to workarounds. Non-Bayesians can do this as well (for example, by boostrapping), but it is not an inherent part of the procedure. But neither Bayesian nor non-Bayesian techniques deal well with the problem of poorly-specificed models and “model non-stationarity”. So what do we do? Using a well thought-through workflow and “generative reasoning” (see 2) can help us iterate towards a specification that describes our data on hand well. Unfortunately, model non-stationarity is a difficult, philosophical problem. There are no quick fixes. Benefit 4: Regularizing richly parameterized models Many important predictive/control variables have high dimensionsionality. For example, imagine you are building a model to predict whether the a package deliverd from town A to B will be delivered late. An important predictive variable will be the origin and destination towns. In most countries, there are a very large number of post-codes, whose values have no intrinsic meaning (so we cannot use these as a numerical control variable). How can we use these values to generate better predictions? And how should we learn from towns with very few observations vis-à-vis towns with many? How do we deal with this problem without using Bayesian methods? One popular technique is to “one-hot” encode the categorical predictors, so that our data go from zip_code other_data 11111 … 11112 … 11113 … 11114 … … … to zip_code 11111 zip_code 11112 zip_code 11113 zip_code 11114 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 … … … … And then continue apace. The problem with such an approach is that our parameter space becomes extremely large, and we might end up doing “noise-mining”—discovering relationships where there are none, simply through bad luck. A trick known as “regularization” can help prevent this, and is considered good practice when you have a model with many variables. It does so by “penalizing” parameter estimates that are not estimated precisely—for instance, those zip codes with very few observations—“shrinking” the parameter estimates towards zero (or some other value). Regularization like this is widely used in machine learning. A difficulty is that most canned routines that implement regularization do not allow for easy incorporation of uncertainty in the parameter estimates. Bayesian methods also employ regularization, through the use of priors. This is because our parameter estimates will always be between the likelihood estimate (typically, the estimates you’ll get from a non-Bayesian approach) and our priors. Strictly, priors should encode the information that we have about parameter estimates before we estimate a model. One valuable type of prior information that we have is “regularization works!” or “mean reversion happens!” These partly motivate the approaches we advocate for modeling hierarchical and panel data, especially the use of hierarchical priors. Benefit 5: Doing away with tests In frequentist statistics, inference is performed through the use of tests. Testing is normally the process of combining model and data to come up with a test statistic, which will have some large-sample limiting distribution under the assumption that the null hypothesis is correct. We then compare the test statistic to that limiting distribution to determine whether there exists sufficient evidence to reject the null hypothesis. Done well, testing can yield useful insights and help guide modeling. But as an approach to workflow and science, testing is difficult to learn and remember, easy to abuse, and with limited data, can result in many erroneous conclusions. In Bayesian analysis, we do not use tests. All inference is conducted by analysing our fitted model (the posterior), which is typically a matrix of draws from the joint distribution of all parameters and predictions. The posterior has a probabilistic interpretation, and consequently if we want to make a probabilistic statement about our fitted model, we only need to count the number of draws from the posterior for which a condition holds, and divide it by the number of draws. This is a far more intuitive and easy-to-use way of conducting inference. 1.2 Models and inference The type of modeling we are doing here is known as generative modeling, typically with fairly strong assumptions about the parametric sampling distributions that give rise to our data. For instance, we might have a normal linear regression model \\[ y_{i} = X_{i}\\beta + \\epsilon\\mbox{ where } \\epsilon \\sim \\mbox{Normal}(0,\\, \\sigma) \\] Which for many scale-location type distributions, can be written as \\[ y_{i} \\sim \\mbox{Normal}(X_{i}\\beta ,\\, \\sigma) \\] Which simply says that \\(y_{i}\\) is generated according to a normal distribution with a location \\(X_{i}\\beta\\), and with residuals of average size \\(\\sigma\\). In the case of the normal distribution, the location is the mean or expected value, and the scale of the residuals is the residual standard deviation. This is the generative model—the model which we say gives rise to_generates_ the data \\(y\\). (Nb. in this case we refer to \\(X\\) as being conditioning information, ie data that is not generated within the model; if we do not know its value out of sample we should be modeling it too!) In this course we’ll restrict ourselves to generative models where the generative distribution has a known parametric form. Don’t be put off by the choice of a normal distribution. If we thought that large deviations from the expected value were not unexpected, we might use a fat-tailed distribution, like \\[ y_{i} \\sim \\mbox{Student&#39;s t}(\\nu, X_{i}\\beta ,\\, \\sigma) \\] or \\[ y_{i} \\sim \\mbox{Cauchy}(X_{i}\\beta ,\\, \\sigma) \\] In any case, the above are models. The models themselves have fixed unknown parameters and, possibly, fixed latent variables, about which we want to conduct probabilistic inference. That is, we want to be able to make probabilistic statements about the true (out of sample) values of the the fixed values of unknown parameters and latent variables. It is common to use different techniques to do this, with the technique depending on the model. For example, Model Estimation technique Inference method Normal linear model OLS Maximum likelihood IV/GMM Appeal to sampling theory/pray Generalized linear model Maximum likelihood/GMM Bootstrap Hierarchical linear model Expectation maximization (EM)/MCMC Approximation to Bayesian/ Genuine Bayes Mixed logit Simulated likelihood Bootstrap Aggregate mixed logit BLP/GMM Bootstrap while reading War and Peace State-space models Kalman filter/particle filters/sequential Bayes Built into method Neural network Gradient descent with backpropagation/Variational Bayes Only care about predictions Gaussian process Maximum likelihood/variational Bayes/MCMC This distinction between models and estimation procedures probably feels quite comfortable for economists. In other fields, particularly nonparametric machine learning, there is no generative model, and the estimation procedure and “model” are one and the same. The point of Stan is to allow us to express a wide range of probability models, such as those above. Once we have expressed the generative model and collected our data, we can use a variety of techniques to estimate the parameters of these models. In principle we can fit models that do not make use of prior information, which corresponds to the maximum likelihood estimate. But normally we’ll find it more useful to use prior information, in which case we’ll be estimating models using Bayesian techniques. Stan implements three cutting-edge estimation methods: NUTS, a variety of Hamiltonian Monte Carlo. This is the default method, and provides amazingly robust estimation of arbitrarily complex models. The only downside is that it can take a little while to run, especially on larger datasets. There are ongoing efforts to speed it up. We’ll provide a very high-level conceptual background below. Variational Bayes. This method approximates the (unconstrained) posterior by minimizing the KL divergence between the true posterior and a Gaussian proposal. Because it’s just an optimization problem, it works more quickly than NUTS. But the quality of the estimates aren’t as robust. Yet for some models, in particular, neural networks, this is less of a concern. Optimization. Stan has three optimizers, a simple Newton method, BFGS, and L-BFGS (which uses less memory and so scales to bigger parameter spaces). These can be used to find the maximum likelihood or penalized likelihood estimates to our models. 1.3 Why use Stan? Once you’ve accepted that you want to have the option of using Bayesian methods to fit your models, you’re left with a choice of how to do it. Roughly, you have a few options: Hand-code a sampler in your scientific language of choice (Julia or C++ good options). Hard! But good for niche uses, a la FRBNY’s DSGE model. Use old-school Gibbs sampling software like BUGS or JAGS. Easy, but limited in the models you can express/size of the parameter space. New-school software, including Stan and PyMC3. These are very similar pieces of software, with the main difference being that Stan ports across languages (R, Python, Matlab, Julia, Mathematica, Stata and the command line), whereas PyMC3 is a package for Python. If you’re a die-hard Python developer, you might prefer using PyMC3. Stan is definitely one of the better contenders among these. It is not the right tool for all jobs, but it is the best tool for many. The jobs where Stan dominates tend to look like: Continuous parameter space. Stan doesn’t estimate discrete parameters (though many discrete parameters can be marginalized out) High dimensioned parameter space, as in latent data or hierarchical models—especially when ratio of the number of parameters to number of observations is high Peculiar likelihoods Useful prior information Not standard models (Run-of-the-mill models often have well-implemented canned routines elsewhere). Because it’s fun and helps you think better. 1.4 Background: Bayes rule, likelihood and priors Bayesian inference is hard in the sense that thinking is hard. - Don Berry Fitting a model using Bayesian techniques involves two steps. The first is working out a likelihood for our data given a set of unknowns, normally within a parametric framework. Recall, we use the term “data” to refer to any variables whose values we do not know out of sample. The second step is to impose prior distributions over the unknowns in the model. If a model is well-posed, the likelihood can be evaluated, and the priors are proper (integrate to 1), then Stan should be able to fit the model. Let’s discuss a bit what these terms mean. 1.4.1 Likelihood/log likelihood Let’s say we have a very simple linear model of \\(y\\) with a single covariate \\(x\\). \\(\\alpha\\), \\(\\beta\\) and \\(\\sigma\\) are the model’s parameters. \\[ y_{i} = \\alpha + \\beta x_{i} + \\epsilon_{i} \\] with \\(\\epsilon_{i} \\sim \\mbox{N}(0, \\sigma)\\). This is the same as saying that \\[ y_{i} \\sim \\mbox{N}(\\alpha + \\beta x_{i}, \\sigma) \\] This is the model we are saying generates the data. It has three unknowns, \\(\\alpha\\), \\(\\beta\\) and \\(\\sigma\\). What we’d ideally want is some way of scoring various combinations of these unknowns so that the score is high when the model describes the data well, and low when it does not. One such way of creating a score is to evaluate the likelihood of the data given the parameters. Let’s say we propose a set of parameters, \\(\\alpha = 2\\), \\(\\beta = .5\\) and \\(\\sigma = 1\\) and we have an obervation \\(y_{i} = 1,\\, x_{i} = 1\\). Given the parameters and \\(x_{i}\\) we know that the outcomes \\(y\\) should be distributed as \\[ y_{i} \\sim \\mbox{N}(\\alpha + \\beta x_{i}, \\sigma) \\] which might look like this: Now we ask: what was the density at the actual outcome \\(y_{i}\\)? The value of the density at an observed outcome is the likelihood contribution \\(f(y_{i} | x_{i}, a, b, s)\\) of a single datapoint. If we assume (conditional) independence of the draws, the sample likelihood of all data is the product of all the individual out of sample likelihoods \\(\\prod_{i = 1}^{N}f(y_{i} | x_{i}, a, b, s)\\). You can probably see the problem here—for reasonably large spreads of the data, the value of the predictive density for a datapoint is typically less than 1, and the product of many such datapoints will be an extremely small number. This might give us numerical issues. One way around this is to take the log of the likelihood—the Log Likelihood. Because the log of numbers in (0, 1) are negative numbers large in absolute value, the sum of these numbers will also have a large absolute value, so we’ll not get numerical issues in computation. \\[ \\log\\left( \\prod_{i = 1}^{N}f(y_{i} | x_{i}, a, b, s)\\right) = \\sum_{i = 1}^{N}\\log(f(y_{i} | x_{i}, a, b, s)) \\] 1.4.2 So what does the likelhood mean? Log likelihood is a confusing concept to beginners as the values of the numbers aren’t easily interpretable. You might run the analysis and get a number of -3477 and ask “is that good? Bad? What does it mean?” These numbers are more meaningful when conducting model comparison, that is, we get the out of sample log likelihood for two different models. Then we can compare them. The usefulness of this approach is best illustrated with an example. Imagine that we have an outlier—rather than \\(y_{j} = 1\\) as in the image above, it is -10. Under the proposed model, we’re essentially saying that the such an outcome is all but impossible. The probability of observing an observation that low or lower is 3.732564310^{-36}. The density of such a point would be 4.695195410^{-35}—a very small number. But its log is large in absolute value: -79.0439385. Compare that with the log likelihood of the original \\(y_{j} = 1\\): -2.0439385. An outlier penalizes the log likelhood far more than a point at the mode. This idea helps us do good modeling: we want to give positive weight to outcomes that happen, and no weight to impossible outcomes. Maximum likelihood Maximum likelihood estimators are simple: if the (log) likelihood is a unidimensional score of how well the data fit the model for a (potentially large) number of parameters, then we can simply run an optimizer that attempts to maximize this score by varying the values of the parameters. Such an estimator is called the maximum likelihood estimator. 1.4.3 Prior distributions Prior distributions summarize our information about the values of parameters before seeing the data. For the uninitiated, this is the scary bit of building a Bayesian model, often because of fears of being biased, or having a poor understanding of how exactly the parameter influences the model. Such fears are often revealed by the choice of extremely diffuse priors, for instance \\(\\beta \\sim \\mbox{Normal}(0, 1000)\\). Don’t be afraid of using priors. You almost always have high quality information about the values parameters might take on. For example: Estimates from previous studies Some unknowns have sign restrictions (fixed costs or discount rates probably aren’t negative; price elasticities of demand probably aren’t positive) The knowledge that regularization can help prevent over-fitting The scale of effects are probably known. Going to college probably won’t increase your income 100000%. Prior distributions should be such that they put positive probabilistic weight on possible outcomes, and no weight on impossible outcomes. In the example above, the standard deviation of the residuals, \\(\\sigma\\) must be positive. And so its prior should not have probabilistic weight below 0. That might guide the choice to a distribution like a half-Normal, half-Cauchy, half-Student’s t, inverse Gamma, lognormal etc. 1.4.4 Bayes rule Bayes rule gives us a method for combining the information from our data (the likelihood) and our priors. It says that the (joint) probability density of our parameter vector \\(\\theta\\) is \\[ \\mbox{p}(\\theta|\\, y) = \\frac{\\mbox{p}(y|\\, \\theta)\\, \\mbox{p}(\\theta)}{\\mbox{p}(y)} \\] where \\(\\mbox{p}(y|\\, \\theta)\\) is the likelihood, and \\(\\mbox{p}(\\theta)\\) is the prior. We call \\(\\mbox{p}(\\theta|\\, y)\\) the posterior. Because \\(\\mbox{p}(y)\\) doesn’t depend on the vector of unknowns, \\(\\theta\\), we often express the posterior up to a constant of proportionality \\[ \\mbox{p}(\\theta|\\, y) \\propto \\mbox{p}(y|\\, \\theta)\\, \\mbox{p}(\\theta) \\] What can you take away from this? A few things: If the prior or likelihood is equal to zero for a given value of \\(\\theta\\), so too will be the posterior If the prior is very peaked, the posterior well be drawn towards the prior If the likelihood is very peaked (as tends to happen when you have many observations per unknown), the posterior will be drawn towards the likelihood estimate. Bayes’ rule also tells us that in order to obtain a posterior, we need to have a prior and a likelihood. 1.4.4.1 What does a posterior look like? In Bayesian inference we do not get point estimates for the unknowns in our models; we estimate their posterior distributions. Ideally, we’d want to be able to make posterior inference by asking questions like “what is the 5th percentile of the marginal posterior of \\(\\theta_{1}\\)?”. This would require that we can analytically derive these statistics from the posterior. Sadly, most posteriors do not have a closed form, and so we need to approximate. The two common types of approximation are: To approximate the posterior with a joint density for which we can analytically evaluate quantiles, expected values etc. This is the approach in Variational Bayes and penalized likelihood. To obtain many independent draws from the posterior, and evaluate quantiles, expected values of those draws. This is the approach in MCMC and ABC. In theory, our posterior \\(p(\\theta | y)\\) is an abstract distribution; in practice (when using MCMC), it’s a matrix of data, where each column corresponds to a parameter, and each row a draw from \\(p(\\theta | y)\\). For instance, the first five rows of our posterior matrix for our linear model above might look like: a b sigma 1.93 0.55 1.10 2.03 0.50 1.16 1.99 0.57 1.10 2.03 0.50 1.04 2.04 0.53 1.01 Each of these parameter combinations implies a different predictive distribution. Consequently, they also imply different predictive likelihoods for our out of sample datapoint \\(y_{j} = 1,x_{j} = 1\\). This is illustrated for 30 posterior replicates below. The density of the data across posterior draws is known as the log posterior density of a datapoint. Unlike the likelihood, we are uncertain of its value and need to perform inference. To illustrate this, let’s look at a histogram of the log posterior density in the above example (but with 500 replicates, not 30): Note that the log posterior density can be evaluated for each datapoint; if we have many datapoints for which we want to evaluate the LPD we simply need to evaluate it for each, and sum across all datapoints. 1.5 HMC and Betancourt Under the hood, Stan implements Hamiltonian Monte Carlo, a variant of MCMC that scales to enormous parameter spaces. Sadly there is no time in this short course to discuss HMC in detail. If you want to learn about this enormously powerful method, I recommend the following: (Mike’s fairly talk—good introduction)[https://www.youtube.com/watch?v=VnNdhsm0rJQ] (Mike’s introductory paper)[https://arxiv.org/abs/1701.02434] (The paper for those who know symplectic geometry)[https://arxiv.org/abs/1701.02434] 1.6 A tour of a Stan program A Stan model is comprised of code blocks. Each block is a place for a certain task. The bold blocks below must be present in all Stan programs (even if they contain no arguments): functions, where we define functions to be used in the blocks below. This is where we will write out a random number generator that gives us draws from our assumed model. data, declares the data to be used for the model transformed data, makes transformations of the data passed in above parameters, declares the unknowns to be estimated, including any restrictions on their values. transformed parameters, often it is preferable to work with transformations of the parameters and data declared above; in this case we define them here. model, where the full probability model is defined. generated quantities, generates a range of outputs from the model (posterior predictions, forecasts, values of loss functions, etc.). We typically write out a Stan program in RStudio (with the .stan suffix), which provides syntax highlighting. Note that the Stan program itself only describes the model—the priors and likelihood, and how the parameters, data and conditioning information interact. We do not tell it how to estimate the model. Stan is a templating language. A fairly small Stan program writes a much larger C++ program, which is in turn compiled. The compiled program takes as inputs the unknowns, and returns the log posterior density. The Stan engine can then use the methods described above to estimate the unknowns. 1.6.1 A hello world example Save a new script as hello_world.stan. In this script we’ll implement a simple linear regression model. // saved as hello_world.stan data { int N; // the number of observations int P; // the number of covariates matrix[N, P] X; // our covariates vector[N] y; // the data outcomes } parameters { vector[P] beta; // regression coefficients real&lt;lower= 0&gt; sigma; // standard deviation of the residuals } model { // priors beta ~ normal(0, 1); sigma ~ cauchy(0, 2); // likelihood y ~ normal(X*beta, sigma); } We’ll do the rest in R. To compile the model, we execute the following: library(rstan) options(mc.cores = parallel::detectCores()) cars &lt;- datasets::cars data_list &lt;- list(N = nrow(cars), P = 2, y = cars$dist, X = cbind(1, cars$speed)) # Compile the model compiled_model &lt;- stan_model(&quot;hello_world.stan&quot;) # Fit with HMC model_fit_mcmc &lt;- sampling(compiled_model, data_list) # Fit with variational Bayes model_fit_vb &lt;- vb(compiled_model, data_list) # Fit with penalized likelihood model_fit_optim &lt;- optimizing(compiled_model, data_list) And there you go—your first Stan program! References "],
["msw.html", "Session 2 Modern Statistical Workflow 2.1 Modern Statistical Workflow 2.2 Tools of the trade: borrowing from software engineering", " Session 2 Modern Statistical Workflow This session introduces the process I recommend for model building, which I call “Modern Statistical Workflow”. 2.1 Modern Statistical Workflow The workflow described here is a template for how to build high-quality, robust models. If you work by it, you will learn models more thoroughly, spot errors more swiftly, and build a much better understanding of economics and statistics than you would under a less rigorous workflow. The workflow is iterative. Typically we start with the simplest possible model, working through each step in the process. Only once we have done each step do we add richness to the model. Building models up like this in an iterative way will mean that you always have a working version of a model to fall back on. The process is: Write out a full probability model. This involves specifying the joint distribution for your parameters/latent variables and the conditional distribution for the outcome data. Simulate some data from the model with assumed values for the parameters (these might be quite different from the “true” parameter values). Estimate the model using the simulated data. Check that your model can recover the known parameters used to simulate the data. Estimate the model parameters conditioned on real data. Check that the estimation has run properly. Run posterior predictive checking/time series cross validation to evaluate model fit. Perform predictive inference. Iterate the entire process to improve the model! Compare models—which model are the observed outcomes more plausibly drawn from? 2.1.1 Example: A simple time-series model of loan repayments Before building any model, it is always worth writing down the questions that we might want to ask. Sometimes, the questions will be relativey simple, like “what is the difference in average wages between men and women?” Yet for most large-scale modeling tasks we want to build models capable of answering many questions. In the case of wages, they may be questions like: If I know someone is male and lives in the South what should I expect their wages to be, holding other personal characteristics constant? How much does education affect wages? Workers with more work experience tend to earn higher wages. How does this effect vary across demographic groups? Does variance in wages differ across demographic groups? As a good rule of thumb, the more questions you want a model to be able to answer, the more complex the model will have to be. The first question above might be answered with a simple linear regression model, the second, a more elaborate model that allows the relationship between experience and wages to vary across demographic groups; the final question might involve modeling the variance of the wage distribution, not just its mean. The example given below introduces a simple autoregressive model of a daily KPI from a microfinance company. We’ll introduce an additional complexity—the fictitious client’s systems are unreliable, and for some days, there are missing values for the KPI. Let’s walk through each step of the workflow, gradually introducing Stan along the way. First, let’s download the data. library(readr) # Download file from: # https://github.com/khakieconomics/half_day_course/blob/master/KPI_data.csv then KPI_data &lt;- read_csv(&quot;KPI_data.csv&quot;) 2.1.2 Step 1: Writing out the probability model The first step of of our workflow is to propose an underlying generative model. It’s helpful to think of a generative model as being a structured random number generator, which when simulated, generates outcomes with a distribution that looks like the distribution of the outcome variable. Once we have decided on the generative model, we then get into the specifics of endogeneity issues etc. In deciding the choice of distribution to use, you should plot a histogram or density of the outcome. For example, we could generate a histogram of the KPI like so: library(ggplot2) ggplot(KPI_data, aes(x = kpi_value)) + geom_histogram() + ggthemes::theme_hc(base_size = 12) + ggtitle(&quot;Histogram of KPI values&quot;) We can also plot the time series library(ggplot2) ggplot(KPI_data, aes(x = date, y = kpi_value)) + geom_line() + ggthemes::theme_hc(base_size = 12) + ggtitle(&quot;Time series plot of KPI values&quot;) As we can see, the KPI seems to have a bit of a bell-shaped curve, does not seem to be (locally) truncated, and is continuous. It’d make sense to choose a density from a family that has such characteristics, like: Normal Student’s t Cauchy Gumbel If we decide to choose a normal density as the data-generating process, and assume that the conditional distribution is an autoregressive model with \\(p\\) lags. \\[ \\mbox{kpi}_{t} \\sim \\mbox{Normal}(\\alpha + \\beta_{1}\\mbox{kpi}_{t-1} + \\dots + \\beta_{p}\\mbox{kpi}_{t-p}, \\sigma) \\] which says that a the value of the KPI is distributed according to a normal distribution with location \\(\\alpha + \\beta_{1}\\mbox{kpi}_{t-1} + \\dots + \\beta_{p}\\mbox{kpi}_{t-p}\\) and scale \\(\\sigma\\). In the case of a normal density, the location is the mean, and the scale is the standard deviation. We prefer to use “location” and “scale” rather than “mean” and “standard deviation” because the terminology can carry across to other densities whose location and scale parameters don’t correspond to the mean or standard deviation. The parameters of the generative model are both “true” and unknown. The entire point is to peform inference in order to get probabilistic estimates of the “true” parameters. 2.1.2.1 Choosing the right generative model Above, we picked out a normal density for the conditional distribution of the KPI. a reasonable first step in modeling our wage series. How did we get to this choice? The choice of distribution to use should depend on the nature of your outcome variables. Two good rules of thumb are: The chosen distribution should not give positive probability to impossible outcomes. For example, wages can’t be negative, and so if we were to use a normal density (which gives positive probability to all outcomes) to model wages, we would be committing an error. If an outcome is outcome is binary or count data, the model should not give weight to non-integer outcomes. And so on. The chosen distribution should give positive weight to plausible outcomes. 2.1.2.2 Choosing priors To complete our probability model, we need to specify priors for the parameters \\(\\alpha\\), \\(\\beta\\) and \\(\\sigma\\). Again, these priors should place positive probabilistic weight over values of the parameters that we consider possible, and zero weight on impossible values (like a negative scale \\(\\sigma\\)). In this case, it is common to assume normal priors for regression coefficients and half-Cauchy or half-Student-t priors on scales. A great discussion of choosing priors is available here. In this case, we don’t know how many lags to include. One approach is to use the so-called Horseshoe prior, a prior that shrinks coefficients on uninformative parameters very strongly towards zero, without having a large effect on the remaining coefficients. This is similar in spirit to LASSO or Ridge Regression. The horseshoe prior is: \\[ \\beta_{i} \\sim \\mbox{Normal}(0, \\tau \\lambda_{i}) \\] with \\[ \\lambda_{i} \\sim \\mbox{Cauchy}_{+}(0, 1) \\] And \\(\\tau\\) either provided by the modeler, or estimated. \\(\\tau\\) is the global shrinkage coefficient, and \\(\\lambda_{i}\\) will take large values when \\(\\beta_{i}\\) is non-zero, and small values otherwise. 2.1.3 Step 2: Simulating the model with known parameters An extremely important step in the workflow is to simulate the model with known values for the unknowns. This mightn’t seem very useful when you’re working with simple models, but as your models become more complex, it’s an extremely step. There are two broad approaches to simulating your model with known parameters. The first is to hard-code the parameter values. This is fairly straightforward, and may be prefered when some parameter combinations result in unidentifiable models but it’s hard to define priors that place zero weight on such parameter combinations. A classic case of this is with time-series models, for which some parameter combinations result in an explosive time-series, but those parameter combinations do not have natural priors that would constrain them to be stationary. The second approach is more robust, but it applies to a smaller set of models. In this approach we draw values of the unknowns directly from the priors, and then simulate data. This approach has several large benefits. First, it helps you get a sense for how reasonable your priors are. Quite recently, some colleagues and I were playing around with latent variables in high-dimensioned classification models of the form \\[ y \\sim \\mbox{categorical}(\\mbox{softmax}(\\theta)) \\] where the softmax of \\(\\theta\\) is \\(\\frac{\\exp(\\theta)}{\\sum\\exp(\\theta)}\\). Let’s say that \\(y\\) can take 100 values and we have a diffuse prior \\(\\theta \\sim \\mbox{normal}(0, 100)\\). What does the distribution of probabilities look like for this process? Unless you think that one outcome is certain and the others have probability zero, a \\(\\mbox{normal}(0, 100)\\) prior is clearly unreasonable. On the other hand, what if we were to use \\(\\mbox{normal}(0, 1)\\)? This is a far more reasonable looking distribution of probabilities. Let’s generate some known parameters and covariates and simulate our AR model. T &lt;- 500 alpha &lt;- 0 beta &lt;- c(0.3, 0, 0, 0, 0, .6) sigma &lt;- 1 Y_raw &lt;- NULL Y_raw[1:6] &lt;- rnorm(6, alpha) for(i in 7:T) { Y_raw[i] &lt;- rnorm(1, alpha + Y_raw[(i-6):(i-1)] %*% beta, sigma) } plot.ts(Y_raw) # Set some values to NA (Stan doesn&#39;t like NA, so we use some weird number) Y_raw[sample(1:T,20)] &lt;- NA plot.ts(Y_raw) Y_raw[is.na(Y_raw)] &lt;- -9 N_missing &lt;- 20 # In R: # Load necessary libraries and set up multi-core processing for Stan options(warn=-1, message =-1) library(dplyr); library(ggplot2); library(rstan); library(reshape2) options(mc.cores = parallel::detectCores()) Next we write out our Stan model. This one is a little involved so let’s step through it slowly // saved as ar_model_missing_data.stan data { int T; // number of observations int P; // number of lags int N_missing; // the number of missing values vector[T] Y_raw; // the time series } parameters { real alpha; vector[P] beta; vector&lt;lower = 0&gt;[P] lambda; real&lt;lower = 0&gt; tau; vector[N_missing] missing; real&lt;lower = 0&gt; sigma; } transformed parameters { vector[T] Y; // matrix[T, P] X; // lags matrix { int count; count = 0; for(t in 1:T) { if(Y_raw[t] == -9) { count = count + 1; Y[t] = missing[count]; } else { Y[t] = Y_raw[t]; } } } for(t in 1:T) { if(t&lt;P) { X[t] = rep_row_vector(0, P); } else { X[t] = Y[(t-P):(t-1)]&#39;; } } } model { // priors lambda ~ cauchy(0, 1); tau ~ cauchy(0, 1); beta ~ normal(0, tau*lambda); sigma ~ cauchy(0, 1); // likelihood Y ~ normal(alpha + X*beta, sigma); } ## # In R ## ## compiled_model &lt;- stan_model(&quot;ar_model_missing.stan&quot;) ## ## data_list &lt;- list(T = T, ## P = 6, ## Y_raw = Y_raw, ## N_missing = N_missing) ## ## model &lt;- sampling(compiled_model, ## data_list, ## iter = 1000, ## chains = 4) ## ## print(model, pars = c(&quot;alpha&quot;, &quot;beta&quot;, &quot;sigma&quot;)) We have now fit our model to the fake data. What has been estimated? 2.1.3.1 What do these fitted objects contain? If you are accustomed to estimating models using ordinary least squares (OLS), maximum likelihood estimates (MLE), or the general method of moments (GMM), then you may expect point estimates for parameters: regression tables contain an estimate of the parameter along with some standard errors. Full Bayesian inference involves averaging over the uncertainty in parameter estimates, that is, the posterior distribution. For a point estimate, Bayesians typically use the mean of the posterior distribution, because it minimizes expected square error in the estimate; the posterior median minimizes expected absolute error. For all but a few models, posterior distributions cannot be expressed analytically. Instead, numerical techniques involving simulation going under the general heading of Monte Carlo methods, are used to estimate quantities of interest by taking draws from the distribution in question. Monte Carlo estimation is quite simple. Let’s say a parameter \\(\\theta\\) is distributed according to some distribution \\(\\mbox{Foo}(\\theta)\\) for which we have no analytical formula, but from which we can simulate random draws. We want to draw statistical inferences using this distribution; we want its mean (expected value), standard deviation, median and other quantiles for posterior intervals, etc. The Monte Carlo method allows us to make these inferences by simply generating many (not necessarily independent) draws from the distribution and then calculating the statistic of interest from those draws. Because these draws are from the distribution of interest, they will tend to come from the higher probability regions of the distribution. For example, if 50% of the posterior probability mass is near the posterior mode, then 50% of the simulated draws (give or take sampling error) should be near the posterior mode. For example, suppose we want to estimate the expectation of \\(\\mbox{Foo}(\\theta)\\), or in other words, the mean of a variable \\(\\theta\\) with distribution \\(\\mbox{Foo}(\\theta)\\). If we take \\(M\\) random draws from \\(\\mbox{Foo}\\), \\[ \\theta^{(1)}, \\ldots, \\theta^{(M)} \\sim \\mbox{Foo}(), \\] then we can estimate the expected value of \\(\\theta\\) (i.e., its posterior mean) as \\[ \\mathbb{E}[\\theta] \\approx \\frac{1}{M} \\sum_{m=1}^{M} \\theta^{(m)}. \\] If the draws \\(\\theta^{(m)}\\) are independent, the result is a sequence of independent and identically distributed (i.i.d.) draws. The mean of a sequence of i.i.d. draws is governed by the central limit theorem, where the standard error on the estimates is given by the standard deviation divided by the square root of the number of draws. Thus standard error decreases as \\(\\mathcal{O}(\\frac{1}{\\sqrt{M}})\\) in the number of independent draws \\(M\\). What makes Bayesian inference not only possible, but practical, is that almost all of the Bayesian inference for event probabilities, predictions, and parameter estimates can be expressed as expectations and carried out using Monte Carlo methods. There is one hitch, though. For almost any practically useful model, not only will we not be able to get an analytical formula for the posterior, we will not be able to take independent draws. Fortunately, all is not lost, as we will be able to take identically distributed draws using a technique known as Markov chain Monte Carlo (MCMC). With MCMC, the draws from a Markov chain in which each draw \\(\\theta^{(m+1)}\\) depends (only) on the previous draw \\(\\theta^{(m)}\\). Such draws are governed by the MCMC central limit theorem, wherein a quantity known as the effective sample size plays the role of the effective sample size in pure Monte Carlo estimation. The effective sample size is determined by how autocorrelated the draws are; if each draw is highly correlated with previous draws, then more draws are required to achieve the same effective sample size. Stan is able to calculate the effective sample size for its MCMC methods and use that to estimate standard errors for all of its predictive quantities, such as parameter and event probability estimates. A fitted Stan object contains a sequence of \\(M\\) draws, where each draw contains a value for every parameter (and generated quantity) in the model. If the computation has converged, as measured by built-in convergence diagnostics, the draws are from the posterior distribution of our parameters conditioned on the observed data. These are draws from the joint posterior distribution; correlation between parameters is likely to be present in the joint posterior even if it was not present in the priors. 2.1.4 Model inspection To address questions 1 and 2 above, we need to examine the parameter draws from the model to check for a few common problems: Lack of mixing. A poorly “mixing” Markov chain is one that moves very slowly between regions of the parameter space or barely moves at all. This can happen if the distribution of proposals is much narrower than the target (posterior) distribution or if it is much wider than the target distribution. In the former case most proposals will be accepted but the Markov chain will not explore the full parameter space whereas in the latter case most proposals will be rejected and the chain will stall. By running several Markov chains from different starting values we can see if each chain mixes well and if the chains are converging on a common distribution. If the chains don’t mix well then it’s unlikely we’re sampling from a well specified posterior. The most common reason for this error is a poorly specified model. Stationarity. Markov chains should be covariance stationary, which means that the mean and variance of the chain should not depend on when you draw the observations. Non-stationarity is normally the consequence of a poorly specified model or an insufficient number of iterations. Autocorrelation. Especially in poorly specified or weakly identified models, a given draw of parameters can be highly dependent on the previous draw of the parameters. One consequence of autocorrelation is that the posterior draws will contain less information than the number of draws suggests. That is, the effective posterior sample size will be much less than the actual posterior sample size. For example, 2000 draws with high autocorrelation will be less informative than 2000 independent draws. Assuming the model is specified correctly, then thinning (keeping only every k-th draw) is one common approach to dealing with highly autocorrelated draws. However, while thinning can reduce the autocorrelation in the draws that are retained it still sacrifices information. If possible, reparameterising the model is a better approach to this problem. (See section 21 of the manual, on Optimizing Stan code). Divergent transitions. In models with very curved or irregular posterior densities, we often get “divergent transitions”. This typically indicates that the sampler was unable to explore certain regions of the distribution and a respecification or changes to the sampling routine may be required. The easiest way of addressing this issue is to use control = list(adapt_delta = 0.99) or some other number close to 1. This will lead to smaller step sizes and therefore more steps will be required to explore the posterior. Sampling will be slower but the algorithm will often be better able to explore these problematic regions, reducing the number of divergent transitions. All of these potential problems can be checked using the ShinyStan graphical interface, which is available in the shinystan R package. You can install it with install.packages(&quot;shinystan&quot;), and run it with launch_shinystan(correct_fit). It will bring up an interactive session in your web browser within which you can explore the estimated parameters, examine the individual Markov chains, and check various diagnostics. More information on ShinyStan is available here. We will confront most of these issues and show how to resolve them in later chapters when we work with real examples. For now just keep in mind that MCMC samples always need to be checked before they are used for making inferences. 2.1.5 Taking the model to real data Now we have fit the model to some fake data and successfully been able to recover “known unknowns”, we can fit the model to our real data. # Set up our data compiled_model &lt;- stan_model(&quot;ar_model_missing.stan&quot;) ## In file included from file7bf404c3e6d.cpp:8: ## In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/StanHeaders/include/src/stan/model/model_header.hpp:4: ## In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/StanHeaders/include/stan/math.hpp:4: ## In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/StanHeaders/include/stan/math/rev/mat.hpp:4: ## In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/StanHeaders/include/stan/math/rev/core.hpp:12: ## In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/StanHeaders/include/stan/math/rev/core/gevv_vvv_vari.hpp:5: ## In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/StanHeaders/include/stan/math/rev/core/var.hpp:7: ## In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/BH/include/boost/math/tools/config.hpp:13: ## In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/BH/include/boost/config.hpp:39: ## /Library/Frameworks/R.framework/Versions/3.3/Resources/library/BH/include/boost/config/compiler/clang.hpp:196:11: warning: &#39;BOOST_NO_CXX11_RVALUE_REFERENCES&#39; macro redefined [-Wmacro-redefined] ## # define BOOST_NO_CXX11_RVALUE_REFERENCES ## ^ ## &lt;command line&gt;:6:9: note: previous definition is here ## #define BOOST_NO_CXX11_RVALUE_REFERENCES 1 ## ^ ## In file included from file7bf404c3e6d.cpp:8: ## In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/StanHeaders/include/src/stan/model/model_header.hpp:4: ## In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/StanHeaders/include/stan/math.hpp:4: ## In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/StanHeaders/include/stan/math/rev/mat.hpp:4: ## In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/StanHeaders/include/stan/math/rev/core.hpp:42: ## /Library/Frameworks/R.framework/Versions/3.3/Resources/library/StanHeaders/include/stan/math/rev/core/set_zero_all_adjoints.hpp:14:17: warning: unused function &#39;set_zero_all_adjoints&#39; [-Wunused-function] ## static void set_zero_all_adjoints() { ## ^ ## In file included from file7bf404c3e6d.cpp:8: ## In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/StanHeaders/include/src/stan/model/model_header.hpp:4: ## In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/StanHeaders/include/stan/math.hpp:4: ## In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/StanHeaders/include/stan/math/rev/mat.hpp:4: ## In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/StanHeaders/include/stan/math/rev/core.hpp:43: ## /Library/Frameworks/R.framework/Versions/3.3/Resources/library/StanHeaders/include/stan/math/rev/core/set_zero_all_adjoints_nested.hpp:17:17: warning: &#39;static&#39; function &#39;set_zero_all_adjoints_nested&#39; declared in header file should be declared &#39;static inline&#39; [-Wunneeded-internal-declaration] ## static void set_zero_all_adjoints_nested() { ## ^ ## In file included from file7bf404c3e6d.cpp:8: ## In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/StanHeaders/include/src/stan/model/model_header.hpp:4: ## In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/StanHeaders/include/stan/math.hpp:4: ## In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/StanHeaders/include/stan/math/rev/mat.hpp:11: ## In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/StanHeaders/include/stan/math/prim/mat.hpp:59: ## /Library/Frameworks/R.framework/Versions/3.3/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/autocorrelation.hpp:17:14: warning: function &#39;fft_next_good_size&#39; is not needed and will not be emitted [-Wunneeded-internal-declaration] ## size_t fft_next_good_size(size_t N) { ## ^ ## In file included from file7bf404c3e6d.cpp:8: ## In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/StanHeaders/include/src/stan/model/model_header.hpp:4: ## In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/StanHeaders/include/stan/math.hpp:4: ## In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/StanHeaders/include/stan/math/rev/mat.hpp:11: ## In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/StanHeaders/include/stan/math/prim/mat.hpp:298: ## In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/StanHeaders/include/stan/math/prim/arr.hpp:39: ## In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/StanHeaders/include/stan/math/prim/arr/functor/integrate_ode_rk45.hpp:13: ## In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/BH/include/boost/numeric/odeint.hpp:61: ## In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/BH/include/boost/numeric/odeint/util/multi_array_adaption.hpp:29: ## In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/BH/include/boost/multi_array.hpp:21: ## In file included from /Library/Frameworks/R.framework/Versions/3.3/Resources/library/BH/include/boost/multi_array/base.hpp:28: ## /Library/Frameworks/R.framework/Versions/3.3/Resources/library/BH/include/boost/multi_array/concept_checks.hpp:42:43: warning: unused typedef &#39;index_range&#39; [-Wunused-local-typedef] ## typedef typename Array::index_range index_range; ## ^ ## /Library/Frameworks/R.framework/Versions/3.3/Resources/library/BH/include/boost/multi_array/concept_checks.hpp:43:37: warning: unused typedef &#39;index&#39; [-Wunused-local-typedef] ## typedef typename Array::index index; ## ^ ## /Library/Frameworks/R.framework/Versions/3.3/Resources/library/BH/include/boost/multi_array/concept_checks.hpp:53:43: warning: unused typedef &#39;index_range&#39; [-Wunused-local-typedef] ## typedef typename Array::index_range index_range; ## ^ ## /Library/Frameworks/R.framework/Versions/3.3/Resources/library/BH/include/boost/multi_array/concept_checks.hpp:54:37: warning: unused typedef &#39;index&#39; [-Wunused-local-typedef] ## typedef typename Array::index index; ## ^ ## 8 warnings generated. KPI_data$kpi_value[is.na(KPI_data$kpi_value)] &lt;- -9 KPI_data$kpi_value[140:149] &lt;- -9 data_list_real &lt;- list(T = nrow(KPI_data), P = 7, Y_raw = KPI_data$kpi_value, N_missing = sum(KPI_data$kpi_value == -9)) model_fit_real &lt;- sampling(compiled_model, data_list_real, iter = 1000) # And now we can plot our forecasts! model_fit_real %&gt;% as.data.frame() %&gt;% select(contains(&quot;Y[&quot;)) %&gt;% reshape2::melt() %&gt;% group_by(variable) %&gt;% summarise(lower = quantile(value, .25), upper = quantile(value, .75), median = median(value)) %&gt;% mutate(actuals = KPI_data$kpi_value, actuals = ifelse(actuals==-9, NA, actuals), date = KPI_data$date) %&gt;% ggplot(aes(x = date)) + geom_ribbon(aes(ymin = lower, ymax = upper), fill = &quot;orange&quot;, alpha = 0.4) + geom_line(aes(y = median)) + ggthemes::theme_hc() + ggtitle(&quot;Median forecast for our KPI series&quot;) 2.2 Tools of the trade: borrowing from software engineering Building economic and statistical models increasingly requires sophisticated computation. This has the potential to improve our modeling, but carries with it risks; as the complexity of our models grows, so too does the prospect of making potentially influential mistakes. The well-known spreadsheet error in Rogoff and Reinhart’s (Cite) paper—a fairly simple error in very public paper—was discovered. Who knows how many other errors exist in more complex, less scruitinized work? Given the ease of making errors that substantively affect our models’ outputs, it makes sense to adopt a workflow that minimizes the risk of such error happening. The set of tools discussed in this section, all borrowed from software engineering, are designed for this purpose. We suggest incorporating the following into your workflow: Document your code formally. At the very least, this will involve commenting your code to the extend where a colleague could read it and not have too many questions. Ideally it will include formal documentation of every function that you write. When you write functions, obey what we might call “Tinbergen’s rule of writing software”: one function, one objective. Try not to write omnibus functions that conduct a large part of your analysis. Writing small, modular functions will allow you to use unit testing, a framework that lets you run a set of tests automatically, ensuring that changing one part of your code base does not break other parts. Use Git to manage your workflow. Git is a very powerful tool that serves several purposes. It can help you back up your work, which is handy. It also allows you to view your codebase at periods when you committed some code to the code base. It lets you experiment on branches, without risking the main (“production”) code base. Finally helps you work in teams; formalizing a code-review procedure that should help catch errors. "],
["vsb.html", "Session 3 A more difficult model", " Session 3 A more difficult model "],
["aggregate-random-coefficients-logit-bayesian-estimation-using-stan.html", "Session 4 Aggregate random coefficients logit: Bayesian estimation using Stan 4.1 A generative model of consumer choice 4.2 Part 2: Fake data simulation 4.3 Part 3: Writing out the model in Stan 4.4 Conclusion", " Session 4 Aggregate random coefficients logit: Bayesian estimation using Stan This session illustrates how to fit aggregate random coefficient logit models in Stan, using generative/Bayesian techniques. It’s far easier to learn and implement than the BLP algorithm, and has the benefits of being robust to mismeasurement of market shares, and giving limited-sample posterior uncertainty of all parameters (and demand shocks). This comes at the cost of an additional assumption: we employ a parametric model of how unobserved product-market demand shocks affect prices, thereby explicitly modeling the endogeneity between prices and observed consumer choices, whose biasing effect on estimates, classical BLP uses instrumental variables to resolve. Specifying the full generative model, therefore, instrumental variables are not strictly needed (though they can certainly still be incorporated). 4.1 A generative model of consumer choice We base our estimation in the standard aggregate random coefficients logit model used in BLP. In this model, customer \\(i\\) in market \\(t\\) has preferences over product \\(j\\) such that their total utility is \\[ V_{ijt} = u_{ijt} + \\epsilon_{ijt} \\mbox{ where } \\epsilon_{ij t} \\sim \\mbox{Gumbel}(.) \\] \\[ u_{ijt} = \\alpha_{i}p_{jt} + X_{jt}\\beta_{i} + \\xi_{jt} \\] This says that each consumer receives utility, \\(u_{ijt}\\), from good \\(j\\) according to its price, \\(p_{jt}\\), its characteristics, \\(X_{jt}\\), some systematic “demand” or “quality” shock applicable to all customers in market \\(t\\), \\(\\xi_{jt}\\), and some iid Gumbel shock, \\(\\epsilon_{ijt}\\). Note that the coefficients \\(\\alpha_{i}\\) and \\(\\beta_{i}\\) are individual specific (and so are individual random effects). Under the above assumptions, the probability that an individual will purchase good \\(j\\) (as in Train (2003)) is given by: \\[ p(u_{ijt}=\\max(u_{it})) = \\frac{\\exp(\\alpha_{i}p_{jt} + X_{jt}\\beta_{i} + \\xi_{jt})}{1 + \\sum_{j}\\exp(\\alpha_{i}p_{jt} + X_{jt}\\beta_{i} + \\xi_{jt})} \\] Where the denominator includes the expected value of the utility of the outside good \\(1 = \\exp(\\mbox{E}[u_{0}])\\). 4.1.1 Generating aggregate sales data from the model To close the model, we need to connect the probability of an individual’s purchasing choice with each good’s market share. Classical BLP uses GMM to match empirical market shares observed in the data to the average probabilities of each good’s being purchased (integrated with respect to the random coefficient distribution). This approach induces measurement error when markets vary in size, however. A market share computed from 100,000 sales across 5 goods is different in distribution from a market share computed from 1,000 or 100 sales. To account for this, we make the assumption that true market shares \\(s_{jt}\\) map to observed sales (which are observed with error) through a multinomial distribution with a given market size. \\[ y_{jt} \\sim \\mbox{Multinomial}(s_{jt}, \\mbox{Market size}_{t}) \\] 4.1.2 Modeling price A second major difference between our model and other approaches is to model the demand shock \\(\\xi_{jt}\\) as an explicit random variable, about which we want to perform inference. The approach we take is to model it as a latent factor, whose value is revealed partly in utilities (and market shares), and partly in prices or any other endogenous product characteristics. Petrin and Train (2010) propose a similar control function approach and show how it can substitute for IVs, but it seems not to have caught on in the literature. In the Bayesian framework, however, it seems to make particularly good sense. To do this in a parametric way, we make the assumption that prices come from a truncated normal distribution, which gives positive weight to non-negative prices. If we suspect the relationship between \\(X\\), \\(\\xi\\) and prices to not be linear, we could use more flexible semi-parametric or non-parametric models. Or we could propose a structural model for the expected value of price, with pricing errors having a parametric distribution. \\[ p_{jt} \\sim \\mbox{Normal}_{+}(X_{jt}\\gamma + \\lambda \\xi_{jt}, \\sigma_{p}) \\] Here, \\(\\lambda\\) is a factor loading on \\(\\xi_{jt}\\). We’ve used a truncated distribution because it places no weight on negative prices. Using this specification, \\(\\xi_{jt}\\) will be correlated with \\(p_{jt}\\) in the utility model. Note that if we have instruments for price, they could easily be included in this model, but they are not strictly necessary as the (linear) endogeneity of prices on the demand shock is accounted for in the model. 4.1.3 Estimating the model from aggregate market-level data At a high level, the trick to estimating this model is to estimate the the distribution of the individual-level coefficients, rather than the actual individual-level coefficients (which we obviously cannot estimate from aggregate data). We do this by reformulating the utility model in terms of fixed and random utility, and passing in the individual random effects \\(z_{i}\\) as data. First, express utility in terms of a fixed and random portion: \\[ \\mbox{E}[u_{ijt}] = \\alpha p_{jt} + X_{jt}\\beta + (p_{jt},\\, X_{jt}&#39;)\\, L\\, z_{i}&#39; + \\xi_{jt} \\] \\(z_{i}\\) is a row vector of \\(P+1\\) independent draws from some distribution, normally a unit normal, and \\(L\\) is the lower triangular Cholesky factorization of \\(\\Sigma\\), which is a \\(P+1 \\times P+1\\) covariance matrix. To be clear, \\(\\Sigma\\) is the covariance matrix of variations in \\((\\alpha_{i},\\, \\beta_{i}&#39;)\\) across customers. If the element of \\(\\Sigma\\) corresponding to price and product characteristic 3 is negative, it means that customers who are more less sensitive to price (assuming all are negative, those whose \\(\\alpha_{i}\\)s are closer to 0) tend to derive less utility from characteristic 3. Good estimates of \\(\\Sigma\\) are what give us good estimates of the distribution of preferences, which is what we ultimately want. Note that if we have information about how markets differ from one another (for instance their demographics), we could include that information in the random effects part of the model. Given this structure, we can estimate the structural parameters (and demand shocks) using the following method: Draw a \\(NS \\times P+1\\) matrix independent shocks \\(z_{t}\\), for some large number \\(NS\\). We normally use the same shocks for every market. For a given draw of the structural parameters \\(\\alpha,\\, \\beta,\\, \\xi_{jt}, \\Sigma\\), for each market for each \\(i\\in 1:NS\\) calculate \\(\\mbox{E}[u_{ijt}]\\) and hence \\(p(u_{ijt}=\\max(u_{it}))\\). Aggregate individual probabilities into predicted market shares \\(s_{jt,\\mathrm{pred}}\\) Model \\(y_{t}\\) and \\(p_{t}\\) as described above. Steps 2 and 3 occur in every iteration (or, if you are using HMC, every leapfrog step) of your model estimation. 4.2 Part 2: Fake data simulation Astute readers will be aware that we always recommend simulating fake data with known parameters for a model. Here we do precisely that. All fake data simulation is in R. The comments should describe what’s going on here. library(ggplot2); library(dplyr) set.seed(5) # Dimensions of the data. NS &lt;- 500 # 500 fake customers in each market J &lt;- 50 # 50 products T &lt;- 20 # 20 markets P &lt;- 3 # 3 covariates # structural parameters alpha &lt;- -1.5 lambda &lt;- .8 beta &lt;- rnorm(P) # Create a covariance matrix of the individual-level parameters scales &lt;- seq(from = .5, to = .9, length.out = P+1) # Generate a random correlation matrix XX &lt;- matrix(rnorm(4*6), 6, 4) Omega &lt;- cor(XX) Sigma &lt;- diag(scales) %*% Omega %*% diag(scales) # Product characteristics matrix X &lt;- matrix(rnorm(J*P), J, P) # Easier to use if we repeat it for each market. We can have different # characteristics (like advertising) in each market X_long &lt;- do.call(rbind, replicate(T, X, simplify = F)) # structural shocks and price xi &lt;- rnorm(T*J, 0, 1) xi_mat &lt;- matrix(xi, T, J, byrow = T) gamma &lt;- rnorm(P) gamma0 &lt;- 3 P2 &lt;- 3 # number of instruments gamma2 &lt;- rnorm(P2) Z &lt;- matrix(rnorm(P2*J*T), J*T, P2) # The generative model for price price &lt;- truncnorm::rtruncnorm(T*J, a = 0, mean = gamma0 + lambda*xi + X_long %*% gamma, sd = .5) price_mat &lt;- matrix(price, T, J, byrow = T) # Market size market_size &lt;- rpois(T, 30000) # Deltas (the fixed part of utility for each product) delta &lt;- alpha*price + X_long %*% beta + xi delta_mat &lt;- matrix(delta[,1], T, J, byrow = T) # random shocks. (alpha_shocks, beta_shocks) = z_t for generation z &lt;- matrix(rnorm(NS*(P+1)), NS, P+1) # Empty market shares. Mat is for the observed products; sales is for all goods including the outside good shares_mat &lt;- matrix(NA, T, J) sales &lt;- matrix(NA, T, J+1) # Loop through each matrix and generate sales for each product for(i in 1:T) { # Latent utility matrix utility &lt;- matrix(NA, NS, J) # Create the random component of the (alpha, beta) vector random_effects &lt;- z %*% chol(Sigma) # monte carlo integration for(n in 1:NS){ utility[n,] &lt;-t( exp(delta_mat[i,] + cbind(price_mat[i,], X) %*% random_effects[n,])) utility[n,] &lt;- utility[n,]/(1 + sum(utility[n,])) } shares_mat[i,] &lt;- colMeans(utility) # Now we&#39;re going to observe the shares with measurement error # Last column is the outside good sales[i,] &lt;- rmultinom(1, market_size[i], c(shares_mat[i,], 1 - sum( shares_mat[i,]))) } It should be pointed out that here \\(\\xi_{jt}\\) and \\(p\\) are correlated. This should introduce endogeneity problems in the model. 4.3 Part 3: Writing out the model in Stan Below we implement the model described above in Stan. A couple of things to look out for in the code: We pass \\(z_{t}\\) in as two sets of shocks, one for \\(\\alpha\\) and one for \\(\\beta\\). There’s no good reason for this. We stack \\(X\\), a \\(J\\times P\\) characteristic matrix, \\(T\\) times. In the DGP above, we assume that a product has the same characteristics in each market. In reality, we would assume that things like advertising would vary across markets. Although we simulate the price above with instruments, below we don’t use the instruments at all for estimation of the model. // our Stan model, saved as vsb.stan // first we define the function that takes data and parameters and returns predicted market shares functions { // calculates shares for a given market row_vector shares(real alpha, vector beta, matrix bigX, matrix Sigma, row_vector xi, matrix z) { matrix[rows(z), cols(xi)+1] utilities; matrix[rows(z), cols(xi)+1] probs; row_vector[cols(xi)+1] shares; // 1. Rather than pass in p and x separately, we&#39;ll pass in bigX = append_col(p, X) // 2. append alpha_shock, beta_shock { matrix[rows(z), cols(xi)] tmp; tmp = rep_matrix((bigX*append_row(alpha, beta) + xi&#39;)&#39;, rows(z)); // replace the append_col wing single values (might speed things up) utilities[1:rows(z), 1:cols(xi)] = tmp + z * cholesky_decompose(Sigma)&#39; * bigX&#39;; utilities[1:rows(z),cols(utilities)] = rep_vector(0.0, rows(z)); for(i in 1:rows(z)) { probs[i] = softmax(utilities[i]&#39;)&#39;; } } for(j in 1:cols(probs)) { shares[j] = mean(col(probs, j)); } return(shares); } } // next define our data data { int NS; // number of individuals in integration int J; // number of products int T; // number of markets int P; // number of features matrix[NS, P+1] z; // normal(0,1) draws of the shocks matrix[T, J] price; // price for each unit int sales[T, J+1]; // unit sales across T markets for J+1 products (inc outside good) matrix[T*J, P] X_repeat; // T Xs stacked on top of each other. This format allows characteristics to vary across markets. real nu; } // next join the product data together into single matrices transformed data { matrix[T*J, P+1] bigX; bigX = append_col(to_vector(price&#39;), X_repeat); } // define parameters parameters { real alpha; vector[P] beta; vector[P] gamma; real gamma0; real&lt;lower = 0&gt; price_scale; matrix[T, J] xi; vector&lt;lower = 0&gt;[P+1] scales; corr_matrix[P+1] Omega; real&lt;lower = 0&gt; lambda; } transformed parameters { cov_matrix[P+1] Sigma; Sigma = quad_form_diag(Omega, scales); } // and the model model { // priors alpha ~ normal(0, 1); beta ~ normal(0, 1); gamma0 ~ normal(0, 3); gamma ~ normal(0, 3); price_scale ~ normal(0, 1); lambda ~ normal(0, 1); to_vector(xi) ~ normal(0, 1); scales ~ normal(0, 1); Omega ~ lkj_corr(nu); // model of price -- this helps pin down xi for (i in 1:rows(to_vector(price&#39;))) { to_vector(price&#39;)[i] ~ normal(gamma0 + X_repeat[i] * gamma + lambda * to_vector(xi&#39;)[i], price_scale) T[0,]; } // model of sales { matrix[T, J+1] pred_shares; for(t in 1:T) { // predicted market shares given data and parameters pred_shares[t] = shares(alpha, beta, bigX[(t*J-J+1):(t*J)], Sigma, xi[t], z); // sales are measured with multinomial measurement error sales[t] ~ multinomial(pred_shares[t]&#39;); } } } # run model --------------------------------------------------------------- # Compile Stan function to check that it generates sensible shares library(rstan) options(mc.cores = parallel::detectCores()) data_list &lt;- list(NS = nrow(z), J = J, T = T, P = P, z = z, price = price_mat, sales = sales, X_repeat = X_long, nu = 3) # Compile the model compiled_model &lt;- stan_model(&quot;vsb.stan&quot;) # For the sake of time, we estimate this using optimization test_optim &lt;- optimizing(compiled_model, data = data_list) Now how did the model go at recapturing known demand shocks? And how about the structural parameters? Omegas &lt;- test_optim$par[grepl(&quot;Omega&quot;, names(test_optim$par))] Scales &lt;- test_optim$par[grepl(&quot;^scale&quot;, names(test_optim$par))] pars &lt;- c(test_optim$par[1:(P+1)], Omegas, Scales) true_values &lt;- c(alpha, beta, as.vector(Omega), scales) data_frame(Estimates = pars, `True values` = true_values, Parameter = c(&quot;alpha&quot;, rep(&quot;beta&quot;, P), rep(&quot;Omega&quot;, (P+1)^2), rep(&quot;scale&quot;, P+1) )) %&gt;% ggplot(aes(x = `True values`, y = Estimates)) + geom_point(aes(colour = Parameter)) + geom_abline(aes(intercept = 0, slope = 1)) + theme_economist() + labs(title = &quot;Estimates and true values\\nof structural parameters&quot;) Voila!. 4.4 Conclusion What we’ve done above is shown that it can be extremely simple to express a random coefficients logit model with measurement error using a latent factor approach, and importantly, without any instruments. This is a pretty radical departure from the most popular methods in the literature. The other big advantage is that, given you have enough computation time, the model can be estimated using Bayesian techniques. In the code above, we could do this by using sampling() rather than optimizing. With the current code, this is quite slow, but does generate robust parameter estimates. If your decision problem requires genuine estimates of uncertainty around the structural parameters, I’d recommend experimenting using optimization, and then using HMC (sampling) for the production run of the model. In my experiments, I’ve found they generate similar point-estimates. "]
]
